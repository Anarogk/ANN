{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48a7df",
   "metadata": {
    "id": "6623bd36"
   },
   "outputs": [],
   "source": [
    "# Write a program to show back propagation network for XOR function with binary input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e5bda32",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1678435050589,
     "user": {
      "displayName": "Adwait Deshpande",
      "userId": "01421131392947329220"
     },
     "user_tz": -330
    },
    "id": "8e5bda32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0], Predicted Output: [0.]\n",
      "Input: [0 1], Predicted Output: [1.]\n",
      "Input: [1 0], Predicted Output: [1.]\n",
      "Input: [1 1], Predicted Output: [0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define the XOR training data\n",
    "input_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "output_data = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Initialize the weights randomly\n",
    "np.random.seed(42)\n",
    "weights0 = 2 * np.random.random((2, 2)) - 1\n",
    "weights1 = 2 * np.random.random((2, 1)) - 1\n",
    "\n",
    "# Define the number of epochs and learning rate\n",
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Train the neural network\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation\n",
    "    layer0 = input_data\n",
    "    layer1 = sigmoid(np.dot(layer0, weights0))\n",
    "    layer2 = sigmoid(np.dot(layer1, weights1))\n",
    "\n",
    "    # Backward propagation\n",
    "    layer2_error = output_data - layer2\n",
    "    layer2_delta = layer2_error * sigmoid_derivative(layer2)\n",
    "\n",
    "    layer1_error = layer2_delta.dot(weights1.T)\n",
    "    layer1_delta = layer1_error * sigmoid_derivative(layer1)\n",
    "\n",
    "    # Update weights\n",
    "    weights1 += learning_rate * layer1.T.dot(layer2_delta)\n",
    "    weights0 += learning_rate * layer0.T.dot(layer1_delta)\n",
    "\n",
    "# Test the neural network\n",
    "layer0 = input_data\n",
    "layer1 = sigmoid(np.dot(layer0, weights0))\n",
    "layer2 = sigmoid(np.dot(layer1, weights1))\n",
    "\n",
    "# Round the predictions to obtain the final binary outputs\n",
    "rounded_predictions = np.round(layer2)\n",
    "\n",
    "# Print the predictions\n",
    "for i in range(len(input_data)):\n",
    "    print(f\"Input: {input_data[i]}, Predicted Output: {rounded_predictions[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "239654da",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1678435064661,
     "user": {
      "displayName": "Adwait Deshpande",
      "userId": "01421131392947329220"
     },
     "user_tz": -330
    },
    "id": "239654da"
   },
   "outputs": [],
   "source": [
    "# This is a Python code for a neural network that learns to solve the XOR problem, which is a classic problem in the field of artificial intelligence. The XOR function returns a 1 if the inputs are different, and 0 if they are the same. The XOR problem is challenging for traditional computing approaches because it is not linearly separable.\n",
    "\n",
    "# Here is an explanation of the code:\n",
    "\n",
    "# The numpy library is imported, which is used for matrix calculations.\n",
    "# Two functions are defined: sigmoid() and sigmoid_derivative(). The sigmoid function is used as the activation function for the neurons in the network, while the sigmoid_derivative function is used to calculate the gradient during backpropagation.\n",
    "# The XOR function is defined, which takes in two inputs and returns a binary output of 0 or 1 depending on whether the inputs are equal or different.\n",
    "# The input and target data are defined. The input_data is a 2D numpy array consisting of all possible inputs for the XOR function. The target_data is a 2D numpy array consisting of the corresponding outputs for the XOR function.\n",
    "# The neural network architecture is defined by specifying the number of input neurons, hidden neurons, and output neurons.\n",
    "# The weights for the neural network are initialized with random values using the np.random.uniform() function. The hidden_weights and output_weights are numpy arrays with dimensions (input_size, hidden_size) and (hidden_size, output_size) respectively.\n",
    "# The learning rate and number of epochs are defined. The learning rate determines how much the weights are adjusted during each iteration of training. The epochs determine how many times the entire dataset is used to train the neural network.\n",
    "# The neural network is trained using backpropagation by iterating through each epoch.\n",
    "# During each epoch, the forward propagation is performed, where the input data is multiplied by the hidden_weights to get the hidden_layer output, and the hidden_layer output is multiplied by the output_weights to get the output_layer output. The sigmoid activation function is applied to both the hidden_layer and output_layer outputs.\n",
    "# During backpropagation, the output_error is calculated by subtracting the target_data from the output_layer. The output_delta is calculated by multiplying the output_error with the sigmoid_derivative of the output_layer. The hidden_error is calculated by multiplying the output_delta with the transpose of the output_weights. The hidden_delta is calculated by multiplying the hidden_error with the sigmoid_derivative of the hidden_layer.\n",
    "# The weights are updated using the learning rate and the delta values. The output_weights are updated by multiplying the transpose of the hidden_layer with the output_delta. The hidden_weights are updated by multiplying the transpose of the input_data with the hidden_delta.\n",
    "# After training, the neural network is tested on new input data by iterating through each input and calculating the output using the sigmoid activation function and the learned weights. The prediction is rounded to the nearest integer, and the actual target output is calculated using the XOR function. The input, prediction, and target output are printed for each input.\n",
    "\n",
    "# This code implements a neural network in Python that uses backpropagation to learn how to solve the XOR problem. The neural network architecture consists of an input layer, a hidden layer, and an output layer. The weights for the neural network are initialized with random values, and the network is trained using a learning rate and a specified number of epochs. After training, the neural network is tested on new input data, and the input, prediction, and target output are printed for each input.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
